---
title: "5주차 강의일지"
excerpt_separator: "LLM"
categories:
  - Post Formats
tags:
  - Post Formats
  - readability
  - standard
---

> 목차  
> [1. LLM 활용법](#1-llm-활용)  
> [2. RAG](#2-rag)  
> [3. LangChain](#3-langchain)  
> [4. MoEs](#4-moes)   

```yaml
GraphDB
```
  
# 1. LLM 활용법  
- **FIne-Tuning**
  - Fine-Tuning은 LLM을 특정 작업이나 주제에 맞춰 추가 훈련하여, 해당 분야에서 더 정확하고 일관된 성능을 내도록 만드는 방법이다. 이를 통해 모델이 특정 목적에 최적화된 답변을 제공할 수 있다.  

- **Prompting**
  - Prompting은 모델을 재훈련하지 않고, 입력 프롬프트를 설계해 원하는 답변 형식이나 정보를 끌어내는 방법이다. 다양한 작업을 유연하게 수행할 수 있다는 장점이 있다.  

# 2. RAG
**RAG**(Retrieval-Augmented Generation)는 **LLM**이 답변을 생성할 때, 외부의 검색 시스템에서 필요한 정보를 실시간으로 불러와 그 데이터를 기반으로 응답을 생성하는 방법이다. 예를 들어 최신 뉴스나 업데이트된 정보가 필요할 때, 모델이 자체 학습된 데이터만으로는 한계가 있기 때문에, RAG는 검색 시스템을 사용해 최신 정보를 모델에 제공하여 더 신뢰도 높은 답변을 생성하도록 한다. 이렇게 함으로써, 모델이 정적인 정보뿐 아니라 동적인 정보에도 접근할 수 있게 된다.

# 3. LangChain
LangChain은 LLM의 사용을 확장해 여러 작업을 체계적으로 수행할 수 있도록 도와주는 프레임워크이다. LLM을 중심으로 데이터에 접근하고, 검색하며, API나 데이터베이스와 상호작용하는 워크플로를 쉽게 구성할 수 있게 한다. LangChain은 특히 RAG와 같은 접근법을 통해 모델이 외부 데이터베이스나 검색 엔진과 연동되어 필요한 정보를 가져올 수 있도록 해 주며, LLM을 다양한 어플리케이션에서 실용적으로 사용할 수 있도록 돕는다.

# 4. MoEs
**MoEs**(Mixture of Experts)는 여러 개의 전문 모델을 결합하여 특정 작업에 최적화된 답변을 제공하는 방식이다.

전문 모델 구성: MoEs는 다양한 주제나 작업에 특화된 여러 하위 모델(전문가)로 구성된다. 각 전문가는 특정 분야에 대해 잘 알고 있다.

선택적 활성화: 입력된 요청에 따라 가장 적합한 전문가만 선택되어 활성화된다. 예를 들어, 번역 요청이 들어오면 번역 전문가만 작동하고, 다른 전문가는 비활성화된다.

효율성: 필요한 전문가만 활성화되므로 계산 리소스를 절약하면서도 높은 성능을 유지할 수 있다.

즉, MoEs는 필요할 때만 적합한 전문 모델을 사용하여 효율적이고 정확한 응답을 생성하는 방법이다.  
