---
title: "3주차 강의일지"
excerpt_separator: "ai and ni"
categories:
  - Post Formats
tags:
  - Post Formats
  - readability
  - standard
---

> 목차  
> [1. LLM의 종류](#1-llm의-종류)  
> [2. RAG란?](#2-rag란)  
> [3. 대규모 모델의 유형](#3-대규모-모델의-유형)    
> [4. 인공지능 모델의 발전 과정](#4-인공지능-모델의-발전-과정)  
> [5. 8-puzzle 문제](#5-8puzzle-문제)  
> [6. KR&R](#6-krr)  
> [7. 여담](#7-여담)   


```yaml
print('AI AND NI')
```
  
# 1. LLM의 종류
  1. SOTA(State Of The Art) LLM  
  SOTA LLM은 State-of-the-Art Large Language Model의 약자로, 현재 기술 수준에서 최고의 성능을 보이는 대규모 언어 모델을 의미한다. 최신 연구와 기술을 반영해 다양한 자연어 처리 작업에서 우수한 성능을 발휘하는 모델이다.
 
  2. sLLM(SLM)  
  sLLM은 Specialized Large Language Model로, 특정 도메인이나 목적에 맞게 설계된 대규모 언어 모델을 뜻한다. 일반적인 LLM과 달리 특정 분야에 특화된 데이터를 학습해 해당 분야의 작업에서 더 높은 성능을 보인다.

# 2. RAG란?
  RAG는 **Retrieval-Augmented Generation**의 약자로, 정보 검색과 생성 모델을 결합한 방식이다. 먼저 필요한 정보를 검색한 후, 이를 바탕으로 답변을 생성하는 방식이다. 최신 정보나 특정 데이터베이스의 자료를 활용할 수 있는 점이 특징이다.


# 3. 대규모 모델의 유형
  - LLM  
  **LLM**은 **Large Language Model**의 약자로, 대규모 데이터로 학습된 자연어 처리 모델을 의미한다. 텍스트 기반 작업에서 높은 성능을 발휘한다.  
  
  - LMM  
  **LMM**은 **Large Multimodal Model**로, 텍스트뿐만 아니라 이미지, 비디오 등 여러 형태의 데이터를 동시에 처리할 수 있는 대규모 모델이다.  
  
  - LAM  
  **LAM은 Large Audio Model**로, 주로 음성 인식이나 생성 작업에 특화된 대규모 모델이다.  
  
  - LWM  
  **LWM**은 **Large Spatial Model**로, 공간적 정보에 초점을 맞춘 대규모 모델이다. 주로 3D 공간 내에서의 물체 인식, 위치 추적 등과 같은 작업에 활용되며, 공간 데이터를 다루는 데 특화되어 있다.  

# 4. 인공지능 모델의 발전 과정
**인공 신경망**(ANN)은 초기 형태의 모델로, 인간의 뇌 구조를 모방한 방식이다. 입력 데이터를 처리하고 출력하는 간단한 구조를 가지고 있지만, 복잡한 문제 해결에는 한계가 있다.
이후 딥러닝이 등장하면서 다층 신경망을 사용하여 더 복잡한 데이터를 처리할 수 있게 되었다. 이로 인해 이미지 인식과 음성 처리 등에서 높은 성능을 발휘하게 된다.
마지막으로 Transformer 모델이 발전하면서 자연어 처리 분야에 혁신을 가져왔다. Transformer는 병렬 처리가 가능해 대규모 데이터를 효율적으로 처리하며, 이후 GPT와 같은 대규모 언어 모델이 등장하게 되었다.

# 5. 8-puzzle 문제
![1__n4hcTM-akUEoWL1i05xVg](https://github.com/user-attachments/assets/2e38efc9-99ff-4d5a-afe1-bd9d1ce59344)  

8-puzzle 문제를 해결하기 위해 여러 가지 탐색 접근 방식이 있다.

**Search**(탐색)는 가능한 모든 상태를 찾는 과정이다. 이 과정에서 **Operation**(작업)은 빈 칸을 이용해 숫자를 이동시키는 행동을 의미한다.

탐색 알고리즘에는 **Depth-First Search**(깊이 우선 탐색)와 **Breadth-First Search**(너비 우선 탐색)가 있다. 깊이 우선 탐색은 가능한 한 깊게 들어가서 노드를 탐색하고, 막히면 이전 노드로 돌아가는 방식이다. 반면 너비 우선 탐색은 현재 노드의 모든 자식 노드를 먼저 탐색한 후 다음 깊이로 넘어간다.

**Blind Search**(맹목 탐색)는 문제에 대한 추가 정보를 사용하지 않고 단순히 상태를 탐색하는 방법이다. **Informed Search**(정보 탐색)는 문제에 대한 정보를 활용해 더 효율적으로 탐색하는 방법으로, 더 나은 경로를 선택할 수 있다. 이때, **Heuristic Search**(휴리스틱 탐색)는 각 상태의 점수를 평가하고 최적의 해결책을 찾기 위해 휴리스틱 함수를 활용하는 방식이다.

이와 함께 각 상태의 **Cost**(비용)는 해당 상태로 이동하는 데 드는 자원을 나타내며, **Experience**(경험)은 이전 탐색에서 얻은 정보를 통해 더 나은 결정을 내리는 데 활용된다. 이러한 요소들을 통해 8-puzzle 문제를 보다 효과적으로 해결할 수 있다.

# 6. KR&R
인간의 뇌에서 **internal language**는 사고와 사고 과정을 표현하는 내부적인 언어로, 사람의 인지와 의사결정을 돕는다. 이와 유사하게 인공지능에서도 **knowledge representation and reasoning**(지식 표현 및 추론) 기술을 사용하여 정보를 저장하고 처리한다.

**Logic**은 형식적인 언어로, 명제를 기반으로 진리값을 사용해 결론을 도출하는 방식이다. 이를 통해 명확하고 일관된 추론을 수행할 수 있다.

**Production Rule**은 "조건-행동" 형태의 규칙으로, 특정 조건이 충족되면 특정 행동을 수행하는 방식이다. 이 방식은 지식 기반 시스템에서 자주 사용되며, 다양한 상황에 대한 처리를 가능하게 한다.

**Semantic Network**는 개체와 개체 간의 관계를 노드와 링크로 표현하는 그래프 구조로, 지식 간의 연관성을 시각적으로 나타낸다. 이를 통해 정보의 의미를 더 잘 이해하고, 연결된 정보를 쉽게 찾을 수 있다.

**Frame**은 개념을 구조화하여 관련된 속성과 관계를 포함하는 데이터 구조이다. 특정 주제나 개념에 대한 정보를 효과적으로 관리하고 이해하는 데 도움을 준다.

이러한 방식들은 인공지능이 정보를 효율적으로 처리하고, 추론을 통해 결정을 내리는 데 중요한 역할을 한다.

# 7. 여담
**MECE**는 Mutually Exclusive, Collectively Exhaustive의 약자로, 경영에서 문제를 분석할 때 사용하는 논리적 분류 방식이다.  
**Mutually Exclusive**는 상호 배타적이라는 뜻으로, 각 항목이 서로 중복되지 않도록 구분하는 것을 말한다.  
**Collectively Exhaustive**는 전체를 포괄한다는 의미로, 모든 가능한 경우를 빠짐없이 포함해야 함을 뜻한다. 즉, MECE는 **중복 없이** 명확하게 분류하면서도 **빠진 부분이 없도록** 문제를 체계적으로 분석하는 방법이다.

